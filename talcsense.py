# -*- coding: utf-8 -*-
"""talcsense.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X9uHomqZ4DvfXCRcc__2cG4baziAzuZt
"""

!git clone 'https://github.com/gevargas/predcompact'

"""# **Analyse prédictive de la présence de TALC dans les produits cosmétiques**

## **1. Chargement et préparation initiale des données**

**Problème addressé :** Les données brutes sont stockées dans un format Excel hébergé sur GitHub, ce qui nécessite un accès et une conversion pour l'analyse.

**Objectif :** Récupérer le dataset depuis le dépôt GitHub et le convertir en format CSV pour faciliter les opérations de traitement et d'analyse avec pandas.

**Stratégie :**
- Clonage du repository GitHub contenant les données
- Lecture du fichier Excel avec spécification du séparateur et de l'encodage
- Conversion immédiate en CSV pour optimiser les lectures ultérieures

**Principe :** Le format CSV est plus léger et plus rapide à charger que Excel, surtout pour des opérations répétées d'analyse.

"""

import pandas as pd

import matplotlib.pyplot as plt
import numpy as np

import seaborn as sns

import warnings
warnings.filterwarnings("ignore")
warnings.filterwarnings("ignore", category=DeprecationWarning)
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, balanced_accuracy_score


# Chargement du fichier => en csv
df = pd.read_excel("/content/predcompact/375_cosmetikwatch_19_08_2025.xlsx")
df.to_csv("/content/predcompact/375_cosmetikwatch_19_08_2025.csv", index=False)

"""**Interprétation :** Le dataset contient les formulations de 375 produits cosmétiques. Les données ont été chargées avec succès et converties en CSV pour faciliter les traitements ultérieurs.

## **2. Exploration des données (EDA - Exploratory Data Analysis)**

**Problème addressé :** Avant toute modélisation, il est crucial de comprendre la structure, la qualité et les caractéristiques des données pour identifier les problèmes potentiels (valeurs manquantes, types incorrects, distribution déséquilibrée).

**Objectif :** Obtenir une vision complète du dataset : dimensions, types de variables, statistiques descriptives, valeurs manquantes et cardinalités.

**Stratégie :** Application systématique des méthodes d'exploration pandas :
- `head()` : aperçu des premières lignes
- `info()` : types de données et mémoire utilisée
- `describe(include='all')` : résumé statistique adapté aux données textuelles (valeur la plus fréquente, nombre d'éléments uniques, fréquence).
- `isna().sum()` : décompte des valeurs manquantes
- `nunique()` : nombre de valeurs distinctes par colonne

**Principe :** Cette phase d'exploration permet d'identifier les axes de nettoyage nécessaires et de détecter les anomalies avant toute transformation.
"""

# PARTIE EDA
print("=== APERÇU DES DONNÉES (df.head()) ===")
df.head()

print("=== INFO SUR LES COLONNES (df.info()) ===")
df.info()
print("=== STATISTIQUES DESCRIPTIVES (df.describe()) ===")
df.describe(include='all')

print("=== VALEURS MANQUANTES PAR COLONNE ===")
df.isna().sum()

print("=== NOMBRE DE VALEURS UNIQUES PAR COLONNE ===")
df.nunique()

"""**Interprétation des résultats de l'EDA :**

**Structure :**
- **375 produits** avec **15 colonnes** descriptives
- **290 produits maquillage**, majoritairement poudres pour le visage
- **121 marques** de 83 groupes cosmétiques

**Qualité des données :**
- **Code EAN : 328 manquantes** → colonne inutilisable, à exclure
- **Gamme : 118 manquantes** → acceptable (produits hors gamme)
- **Autres colonnes : moins de 40 valeurs manquantes** → excellente complétude

**Diversité :**
- **373 formulations uniques sur 375 produits** → chaque produit est quasi-unique
- **26 zones d'application différentes**, **111 types de produits** → colonnes multi-valuées nécessitant un split

**Complexité technique :**
- Plusieurs colonnes contiennent des listes séparées par "/" ou "," (ex: "VISAGE/CORPS")
- Nécessite un traitement de split avant exploitation

**Décisions prises :**
1. Exclure la colonne "Code EAN"
2. Nettoyer et harmoniser les noms
3. Gérer les colonnes multi-valuées (overlays)
4. Vérifier les doublons

## **3. Nettoyage des données**

### **3.1 Suppression des doublons**

**Problème addressé :** Les erreurs de saisie ou imports multiples peuvent générer des entrées dupliquées, ce qui biaise les analyses statistiques (sur-représentation de certains produits) et les modèles prédictifs (apprentissage redondant).

**Objectif :** Identifier et éliminer toutes les lignes strictement identiques pour garantir l'unicité de chaque produit dans le dataset.

**Stratégie :** Utilisation de `duplicated()` pour compter les doublons, puis `drop_duplicates()` pour les supprimer en conservant la première occurrence.

**Principe :** pandas compare toutes les colonnes simultanément pour détecter les lignes parfaitement identiques. La méthode est efficace même sur des datasets volumineux.
"""

# Suppression des doublons
df_clean = df.copy()

nb_doublons = df_clean.duplicated().sum()
print("Nombre de doublons supprimés :", nb_doublons)

df_clean = df_clean.drop_duplicates()

"""**Interprétation :**
- **0 doublon détecté** → excellente qualité de saisie des données
- Chaque produit du dataset est unique (cohérent avec les 375 noms distincts observés en EDA)
- Aucune suppression nécessaire, le dataset est déjà propre
- Cette unicité garantit l'absence de biais dans les modèles (pas de sur-représentation de certains produits)

### **3.2 Harmonisation des noms INCI**

**Problème addressé :** Les noms d'ingrédients peuvent être saisis avec des variations (casse mixte, espaces multiples, espaces avant/après) qui feraient apparaître le même ingrédient comme différent lors de l'analyse.

**Objectif :** Standardiser tous les noms d'ingrédients selon la nomenclature INCI (International Nomenclature of Cosmetic Ingredients) qui impose les majuscules.

**Stratégie :** Application d'une chaîne de transformations :
1. `strip()` : suppression des espaces en début/fin
2. Regex `\s+` → espace simple : normalisation des espaces multiples
3. `upper()` : conversion en majuscules (standard INCI)

**Principe :** La normalisation textuelle est essentielle en NLP (Natural Language Processing) pour garantir que "talc", " TALC" et "Talc  " soient reconnus comme identiques.
"""

# Harmonisation des noms INCI
df_clean["Ingrédients"] = (
    df_clean["Ingrédients"]
    .astype(str)
    .str.strip()
    .str.replace(r"\s+", " ", regex=True)   # supprime espaces multiples
    .str.upper()                             # INCI = majuscules
)

"""**Interprétation :** Tous les noms d'ingrédients sont maintenant en majuscules et sans espaces parasites, ce qui garantit :
- Une recherche fiable (ex: `contains("TALC")` trouvera tous les cas)
- Une vectorisation cohérente (un seul vecteur par ingrédient réel)

### **3.3 Création du Data Dictionary (Dictionnaire de données)**

**Problème addressé :** Pour assurer la reproductibilité et la documentation du projet, il est nécessaire de formaliser la structure du dataset.

**Objectif :** Créer un référentiel documentant chaque colonne : nom, type, cardinalité, complétude et description.

**Stratégie :** Construction d'un DataFrame récapitulatif avec :
- Noms des champs
- Types de données (object, int64, float64, etc.)
- Nombre de valeurs uniques (cardinalité)
- Nombre de valeurs manquantes
- Colonne "Description"

**Principe :** Le data dictionary est un livrable essentiel en Data Science, facilitant la collaboration et la compréhension du dataset par des tiers.
"""

# Version 1 Data Dictionnary
# Dictionnaire qui contient:
# - noms des colonnes
# - type de chaque colonne
# - nombre de valeurs uniques
# - nombre de valeurs manquantes
data_dict = pd.DataFrame({
    "Champ": df_clean.columns,
    "Type": [str(df_clean[col].dtype) for col in df_clean.columns],
    "Nb valeurs uniques": [df_clean[col].nunique() for col in df_clean.columns],
    "Valeurs manquantes": [df_clean[col].isna().sum() for col in df_clean.columns],
    "Description": ""
})

data_dict

"""**Interprétation du Data Dictionary :**

**Qualité des données :**
- **Code EAN : 328 manquantes** → colonne inutilisable
- **Autres colonnes : moins de 118 manquantes** → bonne complétude générale
- 9 colonnes complètes (0 manquante) incluant Ingrédients et Packaging

**Cardinalité :**
- **Faible** (< 50) : Catégorie, Zone, Packaging, Cible → variables catégorielles simples
- **Moyenne** (50-150) : Marque, Groupe, Type de produit → possibilité de regroupement
- **Élevée** (> 300) : Ingrédients, Conseils, Nom → variables textuelles riches

### **3.4 Gestion des colonnes multi-valuées (Overlays)**

**Problème addressé :** Plusieurs colonnes contiennent des listes d'éléments séparés par des délimiteurs mixtes ("/" et ","), rendant impossible leur exploitation directe.

**Objectif :** Transformer ces colonnes textuelles en listes Python exploitables pour des analyses ultérieures (comptages, one-hot encoding, etc.).

**Stratégie :**
1. Identification des colonnes concernées (Type de produit, Zone d'application, Cibles cosmétiques, Conditionnement)
2. Création d'une fonction `split_overlay()` qui :
   - Unifie les délimiteurs ("/" → ",")
   - Découpe en éléments individuels
   - Nettoie les espaces
   - Gère les valeurs manquantes
3. Application à toutes les colonnes identifiées

**Principe :** Le split permet de passer d'une représentation textuelle "Visage/Corps/Mains" à une liste ['Visage', 'Corps', 'Mains'], facilitant les analyses de co-occurrence et la création de variables binaires.
"""

# Overlays

# Colonnes susceptibles d'avoir des overlays
overlay_cols = [
    "Ingrédients",
    "Type(s) de produit - Formulation(s) / Galénique(s)",
    "Zone(s) d'application",
    "Cible(s) cosmétique(s)",
    "Article(s) de conditionnement / Packaging"
]

# Séparation des overlays
def split_overlay(cell):
    if pd.isna(cell):
        return []
    # Uniformiser "/" et ",", puis séparer
    cell = str(cell).replace("/", ",")
    return [x.strip() for x in cell.split(",") if x.strip()]

# Application à toutes les colonnes
for col in overlay_cols:
    df_clean[col + "_list"] = df_clean[col].apply(split_overlay)

# Vérification
df_clean[overlay_cols + [col + "_list" for col in overlay_cols]].head()

"""**Interprétation :**

**Transformation réussie :**
- Les colonnes textuelles avec délimiteurs mixtes ont été converties en listes Python exploitables
- Exemple : "FARD À JOUES/BLUSH/POUDRE" → ['FARD À JOUES', 'BLUSH', 'POUDRE']
- Les colonnes simples (ex: "VISAGE") deviennent des listes à un élément ['VISAGE']

**Observations :**
- **Type de produit** : contient généralement 2 éléments (formulation + galénique)
- **Zone d'application** : majoritairement simple (VISAGE, JOUES) mais parfois multiple
- **Cible cosmétique** : peut contenir plusieurs types de peaux dans une même formule
- **Ingrédients** : transformés en listes d'ingrédients individuels

**Bénéfice :** Ces nouvelles colonnes "_list" permettent maintenant de compter les occurrences, créer des variables binaires (1 colonne par élément) et analyser les co-occurrences entre types de produits, zones, et cibles.

### **3.5 Désagrégation explicite de la liste d'ingrédients**

**Problème adressé :**
La colonne "Ingrédients" contient une liste d'ingrédients stockée sous forme textuelle ou sous forme de listes Python après le pré-traitement. Cette représentation n'est pas directement exploitable par des algorithmes de Machine Learning et ne permet pas d'observer explicitement comment les ingrédients sont transformés en variables numériques.

**Objectif :**
Produire un dataset tabulaire explicite dans lequel chaque ingrédient correspond à une colonne indépendante indiquant sa présence ou son absence dans chaque produit. Cette étape vise à rendre visible et interprétable la transformation de la liste d'ingrédients avant la phase de modélisation.

**Stratégie :**
* Utilisation de la colonne `Ingrédients` obtenue après le nettoyage et le découpage des listes
* Application d'un encodage multi-label (MultiLabelBinarizer) afin de :
  * Identifier l'ensemble des ingrédients uniques du dataset
  * Créer une colonne binaire (0/1) pour chaque ingrédient
* Construction d'un nouveau DataFrame où :
  * Chaque ligne représente un produit
  * Chaque colonne représente un ingrédient
  * Les valeurs indiquent la présence (1) ou l'absence (0) de l'ingrédient

**Principe :**
La désagrégation transforme une information composite (liste d'ingrédients) en variables atomiques indépendantes. Cette représentation binaire permet :
* Une exploitation directe par les modèles de Machine Learning
* Une inspection claire des données
"""

from sklearn.preprocessing import MultiLabelBinarizer

# Sous-dataset contenant uniquement les ingrédients
df_ing = df_clean[["Ingrédients_list"]].copy()

# Initialisation du binariseur
mlb = MultiLabelBinarizer()

ingredients_encoded = pd.DataFrame(
    mlb.fit_transform(df_ing["Ingrédients_list"]),
    columns=mlb.classes_,
    index=df_ing.index
)

df_ingredients_expanded = ingredients_encoded.copy()

# Vérifications
print("Dimensions du dataset désagrégé :", df_ingredients_expanded.shape)
df_ingredients_expanded.head()

"""**Interprétation de la désagrégation :**

**Transformation réussie :**
- Dataset transformé : **375 produits × 1520 ingrédients** (1 colonne par ingrédient unique)
- Valeurs binaires : 0 = absent, 1 = présent

**Observations :**
- **Matrice très creuse** : Majorité de 0 (chaque produit contient peu d'ingrédients)
- **Variabilité des formulations** : Chaque produit a sa propre signature d'ingrédients

### **3.6 Standardisation et clustering des noms d'ingrédients**

**Problème adressé** :
Les 1520 colonnes du dataset désagrégé contiennent des variantes syntaxiques d'un même ingrédient (ex: "TITANIUM DIOXIDE", "TITANIUM DIOXIDE (CI 77891)", "CI 77891" désignent le même composé). Cette redondance crée du bruit dans les modèles et diminue l'interprétabilité.

**Objectif** :
Standardiser les noms des ingrédients en regroupant automatiquement les différentes variantes d’un même ingrédient INCI, afin d’obtenir un dataset nettoyé, plus lisible et plus adapté.

**Stratégie** :
1. **Normalisation** : Supprimer les éléments syntaxiques non-discriminants (parenthèses avec codes CI, crochets [NANO], espaces multiples)
2. **Vectorisation** : Transformer chaque nom normalisé en vecteur TF-IDF de caractères (n-grammes 3-5) pour capturer les similarités orthographiques
3. **Clustering hiérarchique** : Grouper les noms similaires avec un seuil de distance cosinus de 0.25 (AgglomerativeClustering)
4. **Canonisation** : Pour chaque cluster, choisir la forme la plus courte comme nom standard
5. **Fusion** : Agréger les colonnes d'un même cluster par logique OR (un produit contient l'ingrédient si présent dans au moins une variante)
"""

import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering

# 1. Normalisation générique des noms INCI
# (suppression automatique des variantes syntaxiques)

def normalize_inci(name):
    name = name.upper()
    name = re.sub(r"\(.*?\)", "", name)   # enlève ()
    name = re.sub(r"\[.*?\]", "", name)   # enlève []
    name = re.sub(r"\s+", " ", name)      # espaces multiples
    return name.strip()

# 2. Préparation de la liste des ingrédients du dataset

ingredients = pd.DataFrame({
    "original": df_ingredients_expanded.columns
})

ingredients["normalized"] = ingredients["original"].apply(normalize_inci)

print("Nombre d'ingrédients uniques (avant) :", ingredients.shape[0])

# 3. Construire des clusters de noms d’ingrédients

vectorizer = TfidfVectorizer(
    analyzer="char_wb",
    ngram_range=(3, 5)
)

X = vectorizer.fit_transform(ingredients["normalized"])



# 4. Clustering hiérarchique automatique
# (les clusters correspondent à un même INCI COSING)


clustering = AgglomerativeClustering(
    n_clusters=None,
    distance_threshold=0.25,   # seuil raisonnable pour INCI
    linkage="average",
    metric="cosine"
)

ingredients["cluster"] = clustering.fit_predict(X.toarray())

# 5. Définition du nom INCI standard par cluster
# (forme canonique la plus simple / courte)


cluster_to_standard = (
    ingredients
    .groupby("cluster")["normalized"]
    .apply(lambda x: x.sort_values(key=lambda s: s.str.len()).iloc[0])
)

ingredients["standard"] = ingredients["cluster"].map(cluster_to_standard)

# 6. Reconstruction du dataset binaire standardisé

df_ingredients_standardized = pd.DataFrame(
    0,
    index=df_ingredients_expanded.index,
    columns=ingredients["standard"].unique()
)

for _, row in ingredients.iterrows():
    df_ingredients_standardized[row["standard"]] |= df_ingredients_expanded[row["original"]]

# 7. Vérifications finales

print("Nombre de colonnes AVANT standardisation :", df_ingredients_expanded.shape[1])
print("Nombre de colonnes APRÈS standardisation :", df_ingredients_standardized.shape[1])

df_ingredients_standardized.head()

"""**Interprétation des résultats**

Les résultats montrent que la standardisation a effectivement regroupé plusieurs variantes d’un même ingrédient sous un nom INCI unique. Par exemple, différentes formes liées au zinc (ZINC OXIDE, ZINC SULFATE, ZINC STEARATE, etc.) apparaissent désormais comme des colonnes distinctes mais correctement identifiées, chacune représentant un ingrédient chimique précis, et non plus des doublons syntaxiques.

La réduction du nombre de colonnes de 1520 à **1182** confirme que les variantes redondantes ont été fusionnées, tout en conservant les ingrédients réellement différents. Le dataset final est ainsi plus cohérent, plus lisible et mieux structuré pour l’analyse et la modélisation.

### **3.7 Visualisation du dictionnaire sous forme de graphe.**
"""

import plotly.graph_objects as go
import networkx as nx

# 1. Préparation du graphe (identique à ton travail précédent)
top_clusters = ingredients['cluster'].value_counts().head(10).index
subset = ingredients[ingredients['cluster'].isin(top_clusters)]
G = nx.Graph()

for _, row in subset.iterrows():
    G.add_edge(row['original'], row['standard'])

# 2. Calcul des positions des points (plus aéré)
pos = nx.spring_layout(G, k=0.5, iterations=50)

# 3. Création des lignes (liens entre ingrédients)
edge_x = []
edge_y = []
for edge in G.edges():
    x0, y0 = pos[edge[0]]
    x1, y1 = pos[edge[1]]
    edge_x.extend([x0, x1, None])
    edge_y.extend([y0, y1, None])

edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), hoverinfo='none', mode='lines')

# 4. Création des points (les ingrédients)
node_x = []
node_y = []
node_text = []
node_color = []

for node in G.nodes():
    x, y = pos[node]
    node_x.append(x)
    node_y.append(y)
    node_text.append(node)
    # Couleur rouge pour les noms standards, bleu pour les variantes
    if node in subset['standard'].values:
        node_color.append('red')
    else:
        node_color.append('skyblue')

node_trace = go.Scatter(
    x=node_x, y=node_y, mode='markers', hoverinfo='text',
    text=node_text,
    marker=dict(showscale=False, color=node_color, size=15, line_width=2)
)

# 5. Affichage final (Fonctionne à 100% dans Colab)
fig = go.Figure(data=[edge_trace, node_trace],
             layout=go.Layout(
                title='Graphe Interactif des Ingrédients (Plotly)',
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20,l=5,r=5,t=40),
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))
                )

fig.show()

"""**Analyse du Graphe du Dictionnaire** : Ce graphe illustre la phase de standardisation des ingrédients. Les nœuds rouges représentent les concepts INCI cibles (noms simplifiés), tandis que les nœuds bleus représentent les variantes textuelles extraites du dataset initial. Ce regroupement permet de réduire la dimensionnalité du problème et d'éliminer le bruit lié aux différences de saisie (majuscules, codes CI, parenthèses)

## **3.8 Étude approfondie du Dictionnaire de Données**
Pour répondre aux exigences de précision, nous avons décomposé la création du dictionnaire en trois phases scientifiques :

**Phase 1 : Normalisation syntaxique** — Utilisation de Regex pour supprimer le bruit (parenthèses, codes couleurs CI, astérisques).

**Phase 2 : Analyse de similarité** — Vectorisation TF-IDF par n-grammes de caractères pour calculer la proximité textuelle entre les ingrédients.

**Phase 3 : Consolidation par Clustering** — Regroupement des variantes sous un nom INCI standardisé via l'algorithme Agglomerative Clustering.
"""

import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

# Calcul des dimensions réelles de ton travail
avant = df_ingredients_expanded.shape[1]
apres = df_ingredients_standardized.shape[1]

plt.figure(figsize=(9, 6))
labels = ['Ingrédients Bruts\n(Avant clustering)', 'Dictionnaire Standardisé\n(Après clustering)']
values = [avant, apres]

# Création du graphique
bars = plt.bar(labels, values, color=['#ffcccc', '#ccffcc'], edgecolor='black', alpha=0.8)

# Ajouter les chiffres exacts au-dessus des barres
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 10, int(yval), ha='center', va='bottom', fontweight='bold')

plt.title("Impact du clustering sur la dimensionnalité du dataset", fontsize=14)
plt.ylabel("Nombre de colonnes (Ingrédients uniques)")
plt.grid(axis='y', linestyle='--', alpha=0.3)
plt.show()

"""# **4. Modélisation prédictive**

## **4.1 Définition de la variable cible**

**Problème addressé :** Pour construire un modèle supervisé, nous devons définir une variable cible (y) binaire prédictible à partir des features (X).

**Objectif :** Créer une variable binaire indiquant la présence (1) ou l'absence (0) de TALC dans la formulation.

**Stratégie :**
1. Vérification du ratio présence/absence de TALC (équilibre des classes)
2. Création de `target_talc` via `str.contains("TALC")`
3. Retrait du mot "TALC" des ingrédients pour éviter la data leakage

**Principe :**
- **Équilibre des classes :** Un ratio proche de 50/50 évite les problèmes de classes déséquilibrées
- **Data leakage :** Si on laisse "TALC" dans les features, le modèle apprendrait simplement à chercher ce mot au lieu de comprendre les patterns de co-occurrence avec d'autres ingrédients
"""

# Présence du talc dans la formulation des poudres
df_clean["Ingrédients"].str.contains("TALC", na=False).value_counts()

"""On obtient un ratio True/False assez équilibré pour pouvoir choisir le talc comme cible pour le reste.
Le TALC est présent dans plus de la moitié des formulations poudres, cohérent avec son usage traditionnel, matifiant et texturant dans les cosmétiques.
"""

# Création  de la variable cible
df_ml = df_clean.copy()

df_ml["target_talc"] = df_ml["Ingrédients"].str.contains(
    "TALC", na=False
).astype(int)

"""### **4.2 Prévention de la fuite d'information (Data Leakage)**

**Problème addressé :** Si la variable à prédire (présence de TALC) reste dans les features (liste d'ingrédients), le modèle aura une tâche triviale : chercher le mot "TALC" au lieu d'apprendre les patterns réels.

**Objectif :** Retirer toute trace du mot "TALC" de la colonne ingrédients tout en conservant les autres informations.

**Stratégie :** Application de regex pour :
1. Supprimer "TALC" et "TALC*" (variant avec astérisque)
2. Nettoyer les artefacts (crochets, virgules multiples)
3. Vérification finale que "TALC" n'apparaît plus nulle part

"""

# On retire talc des ingrédients pour éviter
# la fuite d'information.
df_ml["Ingrédients_sans_talc"] = (
    df_ml["Ingrédients"]
    .astype(str)
    .str.upper()
    .str.replace(r"TALC\*?", "", regex=True) # supprime TALC ou TALC* partout
    .str.replace(r"[\[\]]", "", regex=True) # enlève [ et ]
    .str.replace(r"\s*,\s*", ",", regex=True) # uniformise les virgules
    .str.strip(", ") # enlève virgules/espaces début/fin
)

# On vérifie la suppression : np.int64(0)
df_ml["Ingrédients_sans_talc"].str.contains("TALC").sum()

"""**Interprétation :**
- Vérification confirmée : 0 occurrence de "TALC" restante
- Les ingrédients propres sont maintenant prêts pour être utilisés dans la partie prédiction.

"""

# On définit les variables.
X = df_ml["Ingrédients_sans_talc"]
y = df_ml["target_talc"]

"""### **4.3 Vectorisation et séparation train/test**

**Problème addressé :** Les algorithmes de Machine Learning ne peuvent pas traiter directement du texte, ils nécessitent des inputs numériques sous forme de vecteurs ou matrices.

**Objectif :** Transformer les listes d'ingrédients en représentation numérique binaire, puis séparer les données en ensembles d'entraînement et de test.

**Stratégie :**
1. **CountVectorizer avec `binary=True`** :
   - Chaque ingrédient unique → une colonne
   - Valeur 1 si présent, 0 si absent
   - Tokenization sur les virgules
2. **Train/test split 80/20** :
   - Stratification sur y pour conserver le ratio TALC
   - Random_state=42 pour la reproductibilité

**Principe :**
- **Vectorisation binaire :** On ne compte pas les occurrences mais seulement la présence (plus pertinent pour des ingrédients)
- **Stratification :** Garantit que train et test ont le même ratio de TALC, évitant les biais d'échantillonnage

"""

# --- Vectorisation (0/1) ---
# 1: ingrédient présent, 0: ingrédient absent
vectorizer = CountVectorizer(
    tokenizer=lambda x: x.split(","),  # séparer les ingrédients par virgule
    binary=True                        # 0 = absent, 1 = présent
)

X_vect = vectorizer.fit_transform(X)

# --- Split train / test ---
X_train, X_test, y_train, y_test = train_test_split(
    X_vect, y, test_size=0.2, random_state=42, stratify=y
)

"""### **4.4 Entraînement des modèles de baseline**

**Problème addressé :** Sans référence, impossible de savoir si un modèle est performant. Il faut établir des baselines simples avant d'explorer des approches complexes.

**Objectif :** Comparer 3 familles d'algorithmes classiques pour identifier lequel est le plus adapté à notre problème de classification binaire.

**Stratégie :** Test de 3 modèles représentatifs :
1. **Régression Logistique** : modèle linéaire, interprétable, rapide
2. **Arbre de Décision** : modèle non-linéaire, crée des règles explicites
3. **Forêt Aléatoire** : ensemble d'arbres, plus robuste mais moins interprétable

**Principe :**
- Comparer des paradigmes différents (linéaire vs non-linéaire, modèle unique vs ensemble)
- Utiliser des métriques adaptées aux classes équilibrées (F1-macro, Balanced Accuracy)
- Prioriser l'interprétabilité pour un contexte réglementaire (cosmétiques)
"""

# --- Définition des modèles ---
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42)
}

# --- Entraînement et calcul des métriques ---
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    results.append({
        "Modèle": name,
        "F1-macro": f1_score(y_test, y_pred, average="macro"),
        "Balanced Accuracy": balanced_accuracy_score(y_test, y_pred)
    })

# --- Affichage des résultats ---
baseline_results = pd.DataFrame(results)
baseline_results

"""**Interprétation des résultats des modèles baseline :**

**Rappel des métriques :**
- **F1-macro** : moyenne harmonique de précision et rappel pour les deux classes (présence et absence de TALC). Score de 1 = parfait, 0 = nul.
- **Balanced Accuracy** : moyenne des taux de reconnaissance par classe. Score de 1 = parfait, 0.5 = aléatoire.

**Résultats comparatifs :**

1. **Decision Tree : 0.879 (F1) / 0.883 (BA)** → **Meilleur modèle**
   - Excellentes performances sur les deux métriques
   - Capacité à capturer les interactions non-linéaires entre ingrédients
   - Crée des règles explicites facilement interprétables

2. **Logistic Regression : 0.851 (F1) / 0.850 (BA)**
   - Bonnes performances malgré l'hypothèse de linéarité
   - Écart de 3% avec Decision Tree
   - Utile pour l'analyse des coefficients individuels

3. **Random Forest : 0.806 (F1) / 0.801 (BA)**
   - Performances inférieures aux deux autres modèles
   - Possiblement en sur-apprentissage ou dataset trop petit pour bénéficier de l'ensemble

**Décision :** Le **Decision Tree** est retenu comme modèle principal avec 88% de performance sur les deux métriques, offrant le meilleur compromis entre performance et interprétabilité pour un contexte réglementaire.

**Validation :** Ces scores >80% confirment que les ingrédients contiennent des signaux prédictifs forts de la présence de TALC.

# **5. Interprétabilité et explicabilité des modèles**

## **5.1 Analyse des coefficients de régression logistique**

**Problème addressé :** Un modèle performant n'est utile que si on peut expliquer SES décisions, surtout dans un contexte réglementé comme la cosmétique.

**Objectif :** Identifier quels ingrédients individuels ont le plus d'influence sur la prédiction de la présence de TALC.

**Stratégie :**
1. Ré-entraînement d'une régression logistique sur tout le dataset
2. Extraction des coefficients (poids) de chaque ingrédient
3. Tri par valeur absolue pour identifier les plus influents
4. Visualisation des top 20 avec orientation (positif/négatif)

**Principe :**
- En régression logistique, chaque coefficient représente la contribution de l'ingrédient au log-odds de la présence de TALC
- **Coefficient positif** : l'ingrédient augmente la probabilité de présence de TALC
- **Coefficient négatif** : l'ingrédient est plutôt associé à l'absence de TALC
- **Valeur absolue** : mesure la force de l'association (indépendamment du sens)
"""

# --- Entraînement régression logistique ---
model = LogisticRegression(max_iter=1000)
model.fit(X_vect, y)

# --- Récupération des coefficients ---
coefs = model.coef_[0]  # tableau des coefficients
features = vectorizer.get_feature_names_out()  # noms des ingrédients

# --- DataFrame pour faciliter l'analyse ---
importance_df = pd.DataFrame({
    "Ingrédient": features,
    "Coefficient": coefs
})

# --- Top 20 ingrédients par importance absolue ---
importance_df["AbsCoeff"] = importance_df["Coefficient"].abs()
importance_df_top = importance_df.sort_values(by="AbsCoeff", ascending=False).head(20)

# --- Visualisation ---
plt.figure(figsize=(10,6))
plt.barh(importance_df_top["Ingrédient"][::-1], importance_df_top["Coefficient"][::-1], color="pink")
plt.xlabel("Coefficient (impact sur la présence de TALC)")
plt.title("Top 20 ingrédients influençant la présence de TALC")
plt.show()

"""**Interprétation des coefficients :**

**Ingrédients à coefficient POSITIF (favorisent la présence de TALC) :**
- **Magnesium myristate, MICA, Nylon-12** : charges minérales et agents liants typiques des poudres compactes traditionnelles
- **CI 77891, CI 77491** : pigments minéraux dispersés dans des matrices TALC+MICA
- **Cohérence formulatoire** : ces ingrédients forment la base des formulations poudres classiques (blush, fonds de teint, ombres à paupières)

**Ingrédients à coefficient NÉGATIF (associés à l'absence de TALC) :**
- **Dimethicone crosspolymer, Dicalcium phosphate** : substituts du TALC dans les formulations "talc-free" (sans talc)
- **Glycerin, Sodium hyaluronate, 2-hexanediol** : ingrédients aqueux typiques des formules liquides/crémeuses, incompatibles avec les poudres sèches

**Observations métier** :
- Coefficients positifs = formulations poudres traditionnelles avec TALC
- Coefficients négatifs = formulations alternatives (liquides ou poudres "talc-free")
- La signature MICA+TALC est confirmée par les données

## **5.2 Extraction de signatures minimales via règles d'association**

**Problème addressé :** L'analyse univariée (coefficients) ignore les **synergies** entre ingrédients. Or, en formulation, c'est souvent la COMBINAISON qui est significative, pas les ingrédients isolés.

**Objectif :** Identifier les combinaisons d'ingrédients (itemsets) qui prédisent fortement la présence de TALC.

**Stratégie :**
1. **Algorithme Apriori** : découvre les itemsets fréquents (combinaisons d'ingrédients apparaissant souvent ensemble)
2. **Génération de règles d'association** : transforme les itemsets en règles "SI [antécédent] ALORS [conséquent]"
3. **Filtrage sur TALC** : ne conserver que les règles concluant à la présence de TALC
4. **Tri par lift** : prioriser les associations les plus fortes

**Principe :**
- **Support** : fréquence de la combinaison (ex: 0.15 = présente dans 15% des produits)
- **Confidence** : probabilité conditionnelle P(TALC | combinaison d'ingrédients)
  - Ex: confidence=0.90 → si ces ingrédients sont présents, TALC l'est dans 90% des cas
- **Lift** : rapport entre confidence observée et probabilité de base de TALC
  - Lift > 1 : association positive (la combinaison augmente la probabilité)
  - Lift = 1 : indépendance
  - Lift < 1 : association négative
"""

!pip install mlxtend

from mlxtend.frequent_patterns import apriori, association_rules

# --- Préparation des données pour Apriori ---
# On prend les ingrédients vectorisés 0/1
X_apriori = pd.DataFrame(X_vect.toarray(), columns=vectorizer.get_feature_names_out())

# On ajoute la colonne TALC comme target
X_apriori["TALC"] = y

# --- Génération des itemsets fréquents ---
frequent_itemsets = apriori(X_apriori, min_support=0.1, use_colnames=True)

# --- Génération des règles d'association ---
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)

# --- Filtrer les règles qui concernent TALC ---
rules_talc = rules[rules['consequents'].apply(lambda x: 'TALC' in x)]

# Affichage
rules_talc[['antecedents', 'consequents', 'support', 'confidence', 'lift']]

# On trie
rules_talc_sorted = rules_talc.sort_values(by='lift', ascending=False)
rules_talc_sorted[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10)

"""**Interprétation des règles d'association :**

**Métriques observées :**
- **Support : 0.10** (10% des produits) → combinaisons suffisamment fréquentes
- **Confidence : 0.73-0.83** → dans 73 à 83% des cas, ces combinaisons prédisent correctement la présence de TALC
- **Lift : 7.2-7.7** → association très forte (probabilité de TALC multipliée par 7 quand ces ingrédients sont présents)


**Enseignements métier :**
- Les **pigments + parfum** forment des signatures fortes de formulations TALC
- Les **combinaisons de pigments colorés** (chromium hydroxide, iron oxides) indiquent des produits de maquillage coloré à base de TALC
- Ces règles peuvent servir à **prédire la présence de TALC sans analyse chimique** (juste la liste INCI)

**Applications pratiques :**
1. **Contrôle qualité** : vérifier la cohérence des formulations, par exemple :
Si un formulateur voit CI 77891 + Limonene mais PAS de TALC → incohérent, erreur possible
Permet de détecter des formulations atypiques nécessitant vérification
2. **Prédiction** : estimer la probabilité de TALC dans un nouveau produit, par exemple : On voit la combinaison, on déduit alors que le TALC est probablement présent
3. **Reformulation** : identifier quels ingrédients remplacer ensemble pour éliminer le TALC, par exemple : Une marque veut passer "talc-free"
les règles montrent : "Si on enlève le TALC, on doit aussi adapter les pigments et parfums associés"
Guide les chimistes sur quels ingrédients modifier ensemble

### **5.3 Visualisation des règles d'association**

**Problème addressé :** Les tableaux de règles d'association sont difficiles à interpréter rapidement. Une représentation visuelle permet de comparer instantanément la force et la fiabilité des associations.

**Objectif :** Visualiser les deux métriques clés (lift et confidence) pour les 10 meilleures règles afin de faciliter l'identification des signatures les plus pertinentes.

**Stratégie :**
1. Sélection des top 10 règles triées par lift
2. Création de deux graphiques en barres horizontales côte à côte :
   - Gauche : classement par **lift** (force de l'association)
   - Droite : classement par **confidence** (fiabilité de la prédiction)
3. Utilisation de palettes de couleurs distinctes (viridis/plasma) pour différencier les métriques

**Principe :**
- **Graphique Lift** : identifie les combinaisons qui augmentent le plus la probabilité de TALC (effet multiplicateur)
- **Graphique Confidence** : montre la fiabilité prédictive de chaque règle (pourcentage de cas corrects)
- La comparaison visuelle permet de repérer les règles excellentes sur les deux métriques (idéales) vs celles fortes sur une seule
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Select the top 10 rules
top_10_rules = rules_talc_sorted.head(10).copy()

# Convert frozenset to string for better plotting labels
top_10_rules['antecedents_str'] = top_10_rules['antecedents'].apply(lambda x: ', '.join(list(x)))

# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

# Plot Lift
sns.barplot(x='lift', y='antecedents_str', data=top_10_rules, ax=axes[0], palette='viridis')
axes[0].set_title('Top 10 Association Rules for TALC (by Lift)')
axes[0].set_xlabel('Lift')
axes[0].set_ylabel('Antecedents')

# Plot Confidence
sns.barplot(x='confidence', y='antecedents_str', data=top_10_rules, ax=axes[1], palette='plasma')
axes[1].set_title('Top 10 Association Rules for TALC (by Confidence)')
axes[1].set_xlabel('Confidence')
axes[1].set_ylabel('') # Keep y-label clear for the second plot

plt.tight_layout()
plt.show()

"""Interprétation du graphique :
Lift (graphique de gauche) : Mesure dans quelle mesure TALC est plus susceptible d’apparaître lorsque l’antécédent (combinaison d’ingrédients) est présent, par rapport à la probabilité que TALC apparaisse seul.
Une valeur de lift supérieure à 1 indique une association positive ; plus le lift est élevé, plus l’association est forte.

Confidence (graphique de droite) : Indique la probabilité que TALC soit présent étant donné que l’antécédent est présent. Par exemple, une confiance de 0,8 signifie que 80 % des produits contenant l’antécédent contiennent également TALC.

Ces graphiques permettent d’identifier visuellement les combinaisons d’ingrédients les plus influentes pour prédire la présence de TALC dans les produits cosmétiques.
"""